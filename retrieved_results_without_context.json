[
    {
        "score": 1.0114336,
        "content": "async def create_unified_trace(request: Request, call_next):\n    try:\n        trace_id = request.headers.get(\"X-Request-ID\", None)\n        langfuse_client = langfuse_context.get()\n        \n        if not trace_id:\n            trace = langfuse_client.trace(\n                id = request_context.get(),\n                name = f\"Trace ID {request_context.get()}\",\n            )\n            \n            token = tracer_context.set(trace)\n            \n            trace_id = trace.id\n            \n        else:\n            trace = langfuse_client.trace(id = trace_id)\n            \n        request.state.trace_id = trace_id\n        \n        response = await call_next(request)\n        \n        response.headers[\"X-Trace-ID\"] = trace_id\n    finally:\n        tracer_context.reset(token)\n    \n    return response",
        "start_line": 19,
        "end_line": 45,
        "file_name": "middleware.py",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py"
    },
    {
        "score": 0.673534,
        "content": "import uuid\nfrom fastapi import Request\nfrom observe_traces.config.context_util import request_context, tracer_context, langfuse_context",
        "start_line": 1,
        "end_line": 3,
        "file_name": "middleware.py",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py"
    },
    {
        "score": 0.664296865,
        "content": "from contextvars import ContextVar\nfrom fastapi import Request\nfrom langfuse.client import StatefulTraceClient, Langfuse",
        "start_line": 1,
        "end_line": 3,
        "file_name": "context_util.py",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/context_util.py"
    },
    {
        "score": 0.531191349,
        "content": "def embedding_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing embedding API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the embedding provider (e.g., \"pinecone\", \"cohere\", \"jina\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract model_name and inputs from args and kwargs\n            model_name = kwargs.get(\"model_name\", \"\")\n            if not model_name and len(args) > 1:\n                model_name = args[1]  # Assuming model_name is second argument\n            \n            inputs = kwargs.get(\"inputs\", kwargs.get(\"texts\", []))\n            if not inputs and len(args) > 2:\n                inputs = args[2]  # Assuming inputs/texts is third argument\n            \n            # Get request ID from context\n            id = request_context.get()\n            trace_id = id\n                \n            # Get provider config\n            provider_config = EMBEDDING_PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for embedding provider: {provider}, falling back to default handling\")\n                # return await func(*args, **kwargs)\n                raise ValueError(f\"No configuration found for embedding provider: {provider}\")\n            \n            start_time = time.perf_counter()\n            \n            try:\n                # Call the original function\n                result = await func(*args, **kwargs)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n                # Process the response based on provider\n                tokens_data = {}\n                if isinstance(result, tuple):\n                    embeddings = result[0] if len(result) > 0 else []\n                    raw_response = result[1] if len(result) > 1 else None\n                    if raw_response:\n                        tokens_data = provider_config[\"token_parser\"](raw_response)\n                else:\n                    # case when function returns entire json response\n                    raw_response = result\n                    embeddings = provider_config[\"embeddings_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n                # Calculate price if token data is available\n                price_data = {}\n                if tokens_data and \"tokens\" in tokens_data:\n                    price_data = provider_config[\"price_calculator\"](\n                        model_name, \n                        tokens_data.get(\"tokens\", 0)\n                    )\n                \n                # Set timezone to IST\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                # logger.info(f\"Embedding trace data: {trace_data}\")\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating embedding generation for trace: {trace_id}\")\n                    span_data = {\n                        \"service_provider\": provider,\n                        \"model_name\": model_name,\n                        \"input\": inputs if isinstance(inputs, list) else [inputs],\n                        \"tokens\": tokens_data,\n                        \"price\": price_data,\n                        \"input_count\": len(inputs) if isinstance(inputs, list) else 1,\n                        \"response_time\": response_time,\n                        \"start_time\": start_time,\n                        \"end_time\": end_time,\n                        \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\"),\n                        \"embedding_dimensions\": len(embeddings[0]) if embeddings and len(embeddings) > 0 else 0,\n                    }\n                    \n                    # logger.info(f\"Generation data: {span_data}\")\n                    await _LangfuseService.create_span_for_embedding(trace_id = trace_id, span_data = span_data, name=f\"{provider.capitalize()} Embeddings Generation\")\n                    # logger.info(f\"Generation created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "start_line": 160,
        "end_line": 257,
        "file_name": "embed_tracer.py",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py"
    },
    {
        "score": 0.516460896,
        "content": "def llm_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing LLM API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the LLM provider (e.g., \"openai\", \"groq\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(self, model_name, system_prompt, user_prompt, user_query, **params):\n            # Generate trace ID if not provided\n            # if not trace_id:\n\n            id = request_context.get()\n            trace_id = id #str(uuid.uuid4())\n                \n            # Get provider config\n            provider_config = PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for provider: {provider}, falling back to default handling\")\n                return await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n            \n           \n            start_time = time.perf_counter()\n            \n            try:\n            \n                result = await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n               \n                if isinstance(result, tuple):\n                    response_data = result[0] if len(result) > 0 else None\n                    raw_response = result[1] if len(result) > 1 else None\n                    # jis hisab se response de rha he funciton ye change krna pdega \n                    llm_response = response_data\n                    tokens_data = provider_config[\"token_parser\"](raw_response) if raw_response else {}\n                else:\n                    # case when function returns entire json response from llm\n                    raw_response = result\n                    llm_response = provider_config[\"response_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n\n                price_data = provider_config[\"price_calculator\"](\n                    model_name, \n                    tokens_data.get(\"input\", 0), \n                    tokens_data.get(\"output\", 0)\n                )\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    generation_data = {\n                            \"model_name\": model_name,\n                            \"service_provider\": provider,\n                            \"input\": user_query,\n                            \"output\": llm_response,\n                            \"tokens\": tokens_data,\n                            \"price\": price_data,\n                            \"system_prompt\": system_prompt,\n                            \"start_time\": datetime.fromtimestamp(start_time),\n                            \"end_time\": datetime.fromtimestamp(end_time),\n                        }\n                    \n                    # logger.info(f\"Generation data: {generation_data}\")\n                    \n                    await _LangfuseService.create_generation_for_LLM(trace_id, generation_data, f\"{provider.capitalize()} Generation\")\n                    # logger.info(f\"Generation created with ID: {generation_id}\")\n                \n                except Exception as e:\n                    raise e\n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n               \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "start_line": 147,
        "end_line": 230,
        "file_name": "llm_tracer.py",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py"
    }
]