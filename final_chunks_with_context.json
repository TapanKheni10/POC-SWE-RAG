[
    {
        "id": "d3c1bd9c-ccbd-47ab-807b-bd7fe8d4c41f",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService\nImport statements and necessary modules for embedding tracing functionality.\n",
        "size": 327,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "8edfc6c9-5348-438b-857b-f3e765abfc77",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 9,
        "end_line": 24,
        "content": "def calculate_pinecone_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"llama-text-embed-v2\": 0.16,\n        \"multilingual-e5-large\": 0.08,\n        \"pinecone-sparse-english-v0\": 0.08\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nThis code snippet defines a function `calculate_pinecone_price` to compute the cost of using Pinecone's embedding models based on the number of tokens processed.  It's part of a larger module handling pricing and tracing for various embedding providers.\n",
        "size": 750,
        "parent-class": null,
        "function_name": "calculate_pinecone_price"
    },
    {
        "id": "4881f815-7d80-4d2b-89be-51b7fae5b014",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 27,
        "end_line": 41,
        "content": "def calculate_cohere_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"embed-english-v3.0\": 0.1,\n        \"embed-multilingual-v3.0\": 0.1,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nThis function calculates the price for Cohere embedding services based on the model used and the number of tokens processed.\n",
        "size": 575,
        "parent-class": null,
        "function_name": "calculate_cohere_price"
    },
    {
        "id": "c3e97bf4-3874-43ee-ad83-d973d937eb63",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 44,
        "end_line": 59,
        "content": "def calculate_jina_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"jina-embeddings-v2-base-en\": 0.05,\n        \"jina-embeddings-v3\": 0.12,\n        \"jina-embeddings-v2-base-code\": 0.05,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nDefines a function `calculate_jina_price` to compute the cost of using Jina embedding models based on token count and model pricing.\n",
        "size": 632,
        "parent-class": null,
        "function_name": "calculate_jina_price"
    },
    {
        "id": "ba984178-b9cf-4f9f-891f-69b498cc83f7",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 62,
        "end_line": 81,
        "content": "def calculate_voyageai_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"voyage-3\": 0.06,\n        \"voyage-3-lite\": 0.02,\n        \"voyage-finance-2\": 0.12,\n        \"voyage-law-2\": 0.12,\n        \"voyage-code-2\": 0.12,\n        \"voyage-code-3\": 0.18,\n        \"voyage-3-large\": 0.18,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nFunction to calculate the price for embeddings from VoyageAI, based on model name and token count.  Part of a larger module defining pricing and token parsing for various embedding providers.\n",
        "size": 784,
        "parent-class": null,
        "function_name": "calculate_voyageai_price"
    },
    {
        "id": "95a88a8a-2e44-45e3-910c-a1c6319de67b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 85,
        "end_line": 92,
        "content": "def parse_pinecone_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from pinecone response\n    if \"usage\" in response_data:\n        usage = response_data.get(\"usage\", {})\n        return {\n            \"tokens\": usage.get(\"total_tokens\", 0),\n        }\n    return {\"tokens\": 0}\nThis code snippet defines a function `parse_pinecone_tokens` that extracts token usage data from a Pinecone embedding API response, part of a larger module handling embedding provider-specific token parsing and pricing calculations.\n",
        "size": 541,
        "parent-class": null,
        "function_name": "parse_pinecone_tokens"
    },
    {
        "id": "dd14f1a9-2d98-479e-8ccd-d0398660e67a",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 95,
        "end_line": 101,
        "content": "def parse_cohere_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from cohere response\n    meta = response_data.get(\"meta\", {})\n    billed_units = meta.get(\"billed_units\", {})\n    return {\n        \"tokens\": billed_units.get(\"input_tokens\", 0),\n    }\nThis function `parse_cohere_tokens` extracts token usage from a Cohere API response, part of a larger module handling embedding provider-specific token parsing and pricing calculations.\n",
        "size": 469,
        "parent-class": null,
        "function_name": "parse_cohere_tokens"
    },
    {
        "id": "fdf5b78f-86df-468f-b036-e0a0f3fa47f9",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 104,
        "end_line": 109,
        "content": "def parse_jina_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from jina response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }\nThis function `parse_jina_tokens` is one of several token parsers within a larger module for tracing embedding API calls.  It specifically extracts token usage data from Jina embedding service responses.\n",
        "size": 430,
        "parent-class": null,
        "function_name": "parse_jina_tokens"
    },
    {
        "id": "5cc05f79-0457-4677-8755-482ff915c0b9",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 112,
        "end_line": 119,
        "content": "def parse_voyageai_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from voyage response\n    usage = response_data.get(\"usage\", {})\n    print(f\"inside decode voyageai tokens : {usage}\")\n\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }\nThis function `parse_voyageai_tokens` is one of several token parsers within a larger module for tracing embedding API calls across different providers (Pinecone, Cohere, Jina, VoyageAI).  It specifically extracts token usage data from VoyageAI API responses.\n",
        "size": 547,
        "parent-class": null,
        "function_name": "parse_voyageai_tokens"
    },
    {
        "id": "9d4fb180-7f7e-4f44-a340-7a2fdd5d408a",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 146,
        "end_line": 157,
        "content": "def register_embedding_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    embeddings_extractor: Callable\n):\n    # Register a new embedding provider with configurations\n    EMBEDDING_PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        # \"embeddings_extractor\": embeddings_extractor\n    }\nThis function allows registering new embedding providers by adding their token parser, price calculator, and embeddings extractor to a central configuration dictionary.\n",
        "size": 582,
        "parent-class": null,
        "function_name": "register_embedding_provider"
    },
    {
        "id": "a85e2dd3-c590-4f5c-a7dc-d3f24ce2dd1e",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 160,
        "end_line": 257,
        "content": "def embedding_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing embedding API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the embedding provider (e.g., \"pinecone\", \"cohere\", \"jina\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract model_name and inputs from args and kwargs\n            model_name = kwargs.get(\"model_name\", \"\")\n            if not model_name and len(args) > 1:\n                model_name = args[1]  # Assuming model_name is second argument\n            \n            inputs = kwargs.get(\"inputs\", kwargs.get(\"texts\", []))\n            if not inputs and len(args) > 2:\n                inputs = args[2]  # Assuming inputs/texts is third argument\n            \n            # Get request ID from context\n            id = request_context.get()\n            trace_id = id\n                \n            # Get provider config\n            provider_config = EMBEDDING_PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for embedding provider: {provider}, falling back to default handling\")\n                # return await func(*args, **kwargs)\n                raise ValueError(f\"No configuration found for embedding provider: {provider}\")\n            \n            start_time = time.perf_counter()\n            \n            try:\n                # Call the original function\n                result = await func(*args, **kwargs)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n                # Process the response based on provider\n                tokens_data = {}\n                if isinstance(result, tuple):\n                    embeddings = result[0] if len(result) > 0 else []\n                    raw_response = result[1] if len(result) > 1 else None\n                    if raw_response:\n                        tokens_data = provider_config[\"token_parser\"](raw_response)\n                else:\n                    # case when function returns entire json response\n                    raw_response = result\n                    embeddings = provider_config[\"embeddings_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n                # Calculate price if token data is available\n                price_data = {}\n                if tokens_data and \"tokens\" in tokens_data:\n                    price_data = provider_config[\"price_calculator\"](\n                        model_name, \n                        tokens_data.get(\"tokens\", 0)\n                    )\n                \n                # Set timezone to IST\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                # logger.info(f\"Embedding trace data: {trace_data}\")\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating embedding generation for trace: {trace_id}\")\n                    span_data = {\n                        \"service_provider\": provider,\n                        \"model_name\": model_name,\n                        \"input\": inputs if isinstance(inputs, list) else [inputs],\n                        \"tokens\": tokens_data,\n                        \"price\": price_data,\n                        \"input_count\": len(inputs) if isinstance(inputs, list) else 1,\n                        \"response_time\": response_time,\n                        \"start_time\": start_time,\n                        \"end_time\": end_time,\n                        \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\"),\n                        \"embedding_dimensions\": len(embeddings[0]) if embeddings and len(embeddings) > 0 else 0,\n                    }\n                    \n                    # logger.info(f\"Generation data: {span_data}\")\n                    await _LangfuseService.create_span_for_embedding(trace_id = trace_id, span_data = span_data, name=f\"{provider.capitalize()} Embeddings Generation\")\n                    # logger.info(f\"Generation created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator\nThis code defines a decorator `embedding_tracing` that wraps embedding API calls, extracting metadata (model name, inputs, tokens, price, timing), and sending it to an observability service (`_LangfuseService`) for tracing.\n",
        "size": 4798,
        "parent-class": null,
        "function_name": "embedding_tracing"
    },
    {
        "id": "7c2f0b07-7d94-49ef-b71f-1aed7abd7848",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService\nImport statements at the beginning of a Python file defining functions for LLM cost calculation and tracing.\n",
        "size": 359,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "e7b54459-e7d0-4861-ad00-6323084ba88f",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 9,
        "end_line": 32,
        "content": "def calculate_openai_price(model_name: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:\n\n    pricing = {\n        \"gpt-3.5-turbo\": {\"input\": 0.15, \"output\": 0.6},\n        \"gpt-4o\": {\"input\": 2.5, \"output\": 10.0},    \n        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.6},\n        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n        \"o1-2024-12-17\": {\"input\": 15, \"output\": 60},\n        'o3-mini-2025-01-31': {\"input\": 1.1, \"output\": 4.40},\n        \"o1-mini-2024-09-12x\": {\"input\": 1.1,  \"output\": 4.40},\n        # when new openai models come then just add them over here \n    }\n    \n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0})  # Default fallback\n    \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n    \n    return {\n        \"input\": input_price , #round(input_price, 6),\n        \"output\": output_price, #round(output_price, 6),\n        \"total\": total_price, #round(total_price, 6)\n    }\nFunction to calculate OpenAI LLM pricing based on model name and token counts.\n",
        "size": 1154,
        "parent-class": null,
        "function_name": "calculate_openai_price"
    },
    {
        "id": "60843aab-2757-4061-921f-11baccc330d7",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 35,
        "end_line": 54,
        "content": "def calculate_anthropic_price(model_name:str, input_tokens:int, output_tokens:int) -> Dict[str, float]:\n\n    pricing = {\n        \"claude-3-7-sonnet-20250219\" : {\"input\" : 3,\"output\" : 15},\n        \"claude-3-5-sonnet-20241022\" : {\"input\" : 3,\"output\" : 15},\n        \"claude-3-5-haiku-20241022\" : {\"input\" : 0.80,\"output\" : 4},\n        \"claude-3-opus-20240229\" : {\"input\" : 15,\"output\" : 75},\n        \"claude-3-haiku-20240307\" : {\"input\" : 0.25,\"output\" : 1.25},\n    }\n\n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0}) #default fallback \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n\n    return {\n        \"input\": input_price, #round(input_price, 6),\n        \"output\": output_price,#round(output_price, 6),\n        \"total\": total_price, #round(total_price, 6)\n    }\nFunction to calculate Anthropic LLM pricing based on model name and token counts.\n",
        "size": 1011,
        "parent-class": null,
        "function_name": "calculate_anthropic_price"
    },
    {
        "id": "a5b3e176-80f6-45e7-a794-2db9d6369ef4",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 57,
        "end_line": 79,
        "content": "def calculate_groq_price(model_name: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:\n\n    pricing = {\n        \"llama-3.3-70b-versatile\": {\"input\": 0.59, \"output\": 0.79},\n        \"gemma2-9b-it\": {\"input\": 0.2, \"output\": 0.2},\n        \"llama-3.1-8b-instant\": {\"input\": 0.05, \"output\": 0.08},\n        \"llama3-70b-8192\": {\"input\": 0.59, \"output\": 0.79},\n        \"llama-guard-3-8b\": {\"input\": 0.2, \"output\": 0.2},\n        \"llama3-8b-8192\": {\"input\": 0.05, \"output\": 0.08},\n        \"mixtral-8x7b-32768\": {\"input\": 0.24, \"output\": 0.24},\n    }\n    \n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0})  # Default fallback\n    \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n    \n    return {\n        \"input\": round(input_price, 6),\n        \"output\": round(output_price, 6),\n        \"total\": round(total_price, 6)\n    }\nFunction to calculate the price of using Groq LLMs based on input and output tokens.  Part of a larger module defining pricing and token parsing for various LLM providers.\n",
        "size": 1160,
        "parent-class": null,
        "function_name": "calculate_groq_price"
    },
    {
        "id": "46cc947b-2a5e-4ade-9aed-26de84625ab8",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 83,
        "end_line": 89,
        "content": "def parse_openai_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"input\": usage.get(\"prompt_tokens\", 0),\n        \"output\": usage.get(\"completion_tokens\", 0),\n        \"total\": usage.get(\"total_tokens\", 0)\n    }\nThis function parses token usage from OpenAI API responses to extract input, output, and total token counts.\n",
        "size": 392,
        "parent-class": null,
        "function_name": "parse_openai_tokens"
    },
    {
        "id": "aeaf51be-d35c-49ce-801b-c84e296a4aea",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 92,
        "end_line": 98,
        "content": "def parse_groq_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"input\": usage.get(\"prompt_tokens\", 0),\n        \"output\": usage.get(\"completion_tokens\", 0),\n        \"total\": usage.get(\"total_tokens\", 0)\n    }\nThis code snippet defines a function `parse_groq_tokens` that extracts token usage information from a Groq LLM API response, parsing prompt and completion tokens for cost calculation within a larger LLM tracing framework.\n",
        "size": 503,
        "parent-class": null,
        "function_name": "parse_groq_tokens"
    },
    {
        "id": "4e7d1b3b-ca5c-4c2e-abdc-9f4eb2f1e8a9",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 100,
        "end_line": 108,
        "content": "def parse_anthropic_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    input_tokens = usage.get(\"input_tokens\", 0)\n    output_tokens = usage.get(\"output_tokens\", 0)\n    return {\n        \"input\": input_tokens,\n        \"output\": output_tokens,\n        \"total\": input_tokens + output_tokens\n    }\nThis function parses token usage from Anthropic LLM API responses to extract input and output token counts for pricing calculations.\n",
        "size": 480,
        "parent-class": null,
        "function_name": "parse_anthropic_tokens"
    },
    {
        "id": "fb9e9982-6777-475b-8b29-3fcb11cd5d8e",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 133,
        "end_line": 144,
        "content": "def register_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    response_extractor: Callable\n):\n    #Register a new LLM provider with configurations here\n    PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        \"response_extractor\": response_extractor\n    }\nFunction to register a new LLM provider with its token parser, price calculator, and response extractor in the `PROVIDER_CONFIGS` dictionary.\n",
        "size": 525,
        "parent-class": null,
        "function_name": "register_provider"
    },
    {
        "id": "d9a99745-d01c-425e-bf15-d3b2ed4a39eb",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 147,
        "end_line": 230,
        "content": "def llm_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing LLM API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the LLM provider (e.g., \"openai\", \"groq\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(self, model_name, system_prompt, user_prompt, user_query, **params):\n            # Generate trace ID if not provided\n            # if not trace_id:\n\n            id = request_context.get()\n            trace_id = id #str(uuid.uuid4())\n                \n            # Get provider config\n            provider_config = PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for provider: {provider}, falling back to default handling\")\n                return await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n            \n           \n            start_time = time.perf_counter()\n            \n            try:\n            \n                result = await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n               \n                if isinstance(result, tuple):\n                    response_data = result[0] if len(result) > 0 else None\n                    raw_response = result[1] if len(result) > 1 else None\n                    # jis hisab se response de rha he funciton ye change krna pdega \n                    llm_response = response_data\n                    tokens_data = provider_config[\"token_parser\"](raw_response) if raw_response else {}\n                else:\n                    # case when function returns entire json response from llm\n                    raw_response = result\n                    llm_response = provider_config[\"response_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n\n                price_data = provider_config[\"price_calculator\"](\n                    model_name, \n                    tokens_data.get(\"input\", 0), \n                    tokens_data.get(\"output\", 0)\n                )\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    generation_data = {\n                            \"model_name\": model_name,\n                            \"service_provider\": provider,\n                            \"input\": user_query,\n                            \"output\": llm_response,\n                            \"tokens\": tokens_data,\n                            \"price\": price_data,\n                            \"system_prompt\": system_prompt,\n                            \"start_time\": datetime.fromtimestamp(start_time),\n                            \"end_time\": datetime.fromtimestamp(end_time),\n                        }\n                    \n                    # logger.info(f\"Generation data: {generation_data}\")\n                    \n                    await _LangfuseService.create_generation_for_LLM(trace_id, generation_data, f\"{provider.capitalize()} Generation\")\n                    # logger.info(f\"Generation created with ID: {generation_id}\")\n                \n                except Exception as e:\n                    raise e\n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n               \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator\nThis code defines a decorator `llm_tracing` that wraps LLM API calls, capturing metrics like execution time, token usage, and cost, and sending this data to an observability service via `_LangfuseService.create_generation_for_LLM`.\n",
        "size": 3972,
        "parent-class": null,
        "function_name": "llm_tracing"
    },
    {
        "id": "b9491662-0ee3-422c-a7fb-5ddc9c022e38",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService\nImport statements at the beginning of a Python module defining functions for reranking API call tracing.\n",
        "size": 355,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "280bb0a0-ee3a-4dcf-ab75-21b3ab9f552e",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 9,
        "end_line": 20,
        "content": "def calculate_pinecone_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"pinecone-rerank-v0\": 0.10,  # Example: $0.10 per 1k rerank units\n    }\n    rerank_units = tokens_data.get(\"rerank_units\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (rerank_units / 1000) * model_price\n    return {\n        \"rerank_units\": rerank_units,\n        \"price_per_1K\": model_price,\n        \"total\": total_price\n    }\nFunction to calculate the price of Pinecone reranking based on the number of rerank units used.  Part of a larger module defining pricing and token parsing for various reranking providers.\n",
        "size": 666,
        "parent-class": null,
        "function_name": "calculate_pinecone_rerank_price"
    },
    {
        "id": "42f894a3-fbf2-4886-806f-c2b472c84ad0",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 23,
        "end_line": 34,
        "content": "def calculate_cohere_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"rerank-english-v3.0\": 0.15,  # Example: $0.15 per search unit\n    }\n    search_units = tokens_data.get(\"search_units\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = search_units * model_price\n    return {\n        \"search_units\": search_units,\n        \"price_per_unit\": model_price,\n        \"total\": total_price\n    }\nFunction to calculate the price of Cohere reranking based on the number of search units consumed.  Part of a larger module defining pricing and token parsing for various reranking providers.\n",
        "size": 656,
        "parent-class": null,
        "function_name": "calculate_cohere_rerank_price"
    },
    {
        "id": "418692d6-0388-4d52-9f6b-79b08a3ee162",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 37,
        "end_line": 48,
        "content": "def calculate_jina_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"jina-rerank-v1-tiny-en\": 0.08,  # Example: $0.08 per 1M tokens\n    }\n    tokens = tokens_data.get(\"tokens\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (tokens / 1000000) * model_price\n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nFunction to calculate the price of Jina reranking based on token count and predefined pricing.\n",
        "size": 539,
        "parent-class": null,
        "function_name": "calculate_jina_rerank_price"
    },
    {
        "id": "d738e3ba-55af-403e-be3d-d9f2a6abac30",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 51,
        "end_line": 62,
        "content": "def calculate_voyage_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"voyage-rerank-v1\": 0.12,  # Example: $0.12 per 1M tokens\n    }\n    tokens = tokens_data.get(\"tokens\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (tokens / 1000000) * model_price\n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }\nFunction to calculate the price of reranking using the Voyage provider, based on the number of tokens processed.  Part of a larger module defining pricing and token parsing for various reranking services.\n",
        "size": 645,
        "parent-class": null,
        "function_name": "calculate_voyage_rerank_price"
    },
    {
        "id": "39cb354d-60d2-4ed7-a9ca-f28cb92c79d6",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 66,
        "end_line": 68,
        "content": "def parse_pinecone_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\"rerank_units\": usage.get(\"rerank_units\", 0)}\nThis function parses the response from Pinecone's reranking API to extract the number of rerank units used.  It's part of a larger system for tracking and pricing reranking operations across multiple providers.\n",
        "size": 395,
        "parent-class": null,
        "function_name": "parse_pinecone_rerank_tokens"
    },
    {
        "id": "9d60bc6a-1d62-41a3-8b55-2309ad350137",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 71,
        "end_line": 77,
        "content": "def parse_cohere_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from cohere response\n    meta = response_data.get(\"meta\", {})\n    billed_units = meta.get(\"billed_units\", {})\n    return {\n        \"search_units\": billed_units.get(\"search_units\", 0),\n    }\nDefines a function `parse_cohere_rerank_tokens` to extract token usage data from a Cohere reranking API response for cost calculation.\n",
        "size": 431,
        "parent-class": null,
        "function_name": "parse_cohere_rerank_tokens"
    },
    {
        "id": "aab262da-2f21-4819-817e-de5c1f764122",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 80,
        "end_line": 85,
        "content": "def parse_jina_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from jina response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }\nThis code snippet defines a function `parse_jina_rerank_tokens` that extracts token usage data from a Jina reranking API response for cost calculation and tracing.\n",
        "size": 397,
        "parent-class": null,
        "function_name": "parse_jina_rerank_tokens"
    },
    {
        "id": "9eaa3260-1370-4993-8240-1822a799be2c",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 88,
        "end_line": 93,
        "content": "def parse_voyage_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from voyage response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }\nFunction to parse token usage from Voyage reranking API response.  Part of a larger module defining functions for various reranking providers.\n",
        "size": 380,
        "parent-class": null,
        "function_name": "parse_voyage_rerank_tokens"
    },
    {
        "id": "6874464d-fb87-4612-8e6e-e0e240c8ac06",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 120,
        "end_line": 131,
        "content": "def register_reranking_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    rerank_results_extractor: Callable\n):\n    # Register a new reranking provider with configurations\n    RERANKING_PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        \"rerank_results_extractor\": rerank_results_extractor\n    }\nFunction to register a new reranking provider with its token parser, price calculator, and rerank results extractor into a global configuration dictionary.\n",
        "size": 579,
        "parent-class": null,
        "function_name": "register_reranking_provider"
    },
    {
        "id": "5ca7727e-99a1-468d-8fe0-b6b7c67b18b9",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 134,
        "end_line": 248,
        "content": "def reranking_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing reranking API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the reranking provider (e.g., \"pinecone\", \"cohere\", \"jina\", \"voyage\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract model_name, query, and documents from args and kwargs\n            model_name = kwargs.get(\"model_name\", \"\")\n            if not model_name and len(args) > 1:\n                model_name = args[1]  # Assuming model_name is second argument\n            \n            query = kwargs.get(\"query\", \"\")\n            if not query and len(args) > 2:\n                query = args[2]  # Assuming query is third argument\n            \n            documents = kwargs.get(\"documents\", [])\n            if not documents and len(args) > 3:\n                documents = args[3]  # Assuming documents is fourth argument\n\n            top_n = kwargs.get(\"top_n\", 0)\n            if not top_n and len(args) > 4:\n                top_n = args[4]\n            \n            # Get request ID from context\n            id = request_context.get()\n            trace_id = id\n                \n            # Get provider config\n            provider_config = RERANKING_PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for reranking provider: {provider}, falling back to default handling\")\n                return await func(*args, **kwargs)\n            \n            start_time = time.perf_counter()\n            ist = timezone(timedelta(hours=5, minutes=30))\n            \n            try:\n                # Call the original function\n                result = await func(*args, **kwargs)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n                # Process the response based on provider\n                tokens_data = {}\n                if isinstance(result, tuple):\n                    rerank_results = result[0] if len(result) > 0 else []\n                    raw_response = result[1] if len(result) > 1 else None\n                    if raw_response:\n                        tokens_data = provider_config[\"token_parser\"](raw_response)\n                else:\n                    # case when function returns entire json response\n                    raw_response = result\n                    rerank_results = provider_config[\"rerank_results_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n                \n                rerank_results = []\n                for doc in result[\"data\"]:\n                    rerank_results.append(\n                        documents[doc[\"index\"]]\n                    )\n                \n                # Calculate price if token data is available\n                price_data = {}\n                if tokens_data and \"tokens\" in tokens_data:\n                    price_data = provider_config[\"price_calculator\"](\n                        model_name, \n                        tokens_data.get(\"tokens\", 0)\n                    )\n\n                # raw_response = result\n                # tokens_data = provider_config[\"token_parser\"](raw_response)\n                # rerank_results = provider_config[\"rerank_results_extractor\"](raw_response)\n                # price_data = provider_config[\"price_calculator\"](model_name, tokens_data)\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    span_data = {\n                        \"service_provider\": provider,\n                        \"model_name\": model_name,\n                        \"tokens\": tokens_data,\n                        \"price\": price_data,\n                        \"query\": query,\n                        \"documents\": documents,\n                        \"document_count\": len(documents),\n                        \"top_n\": top_n,\n                        \"rerank_results\": rerank_results,\n                        \"response_time\": response_time,\n                        \"start_time\" : start_time,\n                        \"end_time\" : end_time,\n                        \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\")\n                    }\n                    \n                    # logger.info(f\"Span data: {span_data}\")\n                    await _LangfuseService.create_span_for_reranking(trace_id = trace_id, span_data=span_data, name=f\"{provider.capitalize()} Reranking\")\n                    # logger.info(f\"Span created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator\nThis code defines a decorator `reranking_tracing` that instruments reranking API calls for different providers (Pinecone, Cohere, Jina, Voyage), logging relevant metrics and sending trace data to a Langfuse service.\n",
        "size": 5315,
        "parent-class": null,
        "function_name": "reranking_tracing"
    },
    {
        "id": "8a56d7a4-1125-4571-9a3d-704e8fe3874b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService\nImport statements at the beginning of a Python module defining functions for vector database tracing.\n",
        "size": 342,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "14330138-a2ee-478f-88e6-bd1d67024150",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 10,
        "end_line": 21,
        "content": "def calculate_pinecone_price(operation_type: str, units: int) -> Dict[str, float]:\n    pricing = {\n        \"read\": 16.0,  # $16 per million read units\n        \"write\": 4.0,  # $4 per million write units\n    }\n    \n    price = (units / 1000000) * pricing[operation_type]\n    \n    return {\n        \"units\": units,\n        \"price\": price\n    }\nFunction to calculate Pinecone vector database pricing based on operation type and units.\n",
        "size": 431,
        "parent-class": null,
        "function_name": "calculate_pinecone_price"
    },
    {
        "id": "024e3100-03e5-4b39-81d5-7db6e2b59aba",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 24,
        "end_line": 28,
        "content": "def parse_pinecone_write_response(response_data: Dict[str, Any]) -> Dict[str, int]:\n    return {\n        \"operation_type\": \"write\",\n        \"units\": response_data.get(\"upsertedCount\", 0)\n    }\nFunction to parse Pinecone write response and extract relevant data for pricing and tracing.\n",
        "size": 286,
        "parent-class": null,
        "function_name": "parse_pinecone_write_response"
    },
    {
        "id": "f61e408c-bb1d-4a1c-86e0-15efe9d7e019",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 30,
        "end_line": 36,
        "content": "def parse_pinecone_read_response(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    # print(f\"usage in pinecone parse read response : {usage}\")\n    return {\n        \"operation_type\": \"read\",\n        \"units\": usage.get(\"readUnits\", 0)\n    }\nThis function parses the response from a Pinecone vector database read operation to extract the number of read units used.  It's part of a larger module handling vector database tracing and pricing.\n",
        "size": 485,
        "parent-class": null,
        "function_name": "parse_pinecone_read_response"
    },
    {
        "id": "82545d4a-4994-4116-ac44-31c0e702fd72",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 56,
        "end_line": 155,
        "content": "def vectordb_tracing(provider: str, operation_type: str):\n    \"\"\"\n    Decorator for tracing Vector DB API calls\n    \n    Args:\n        provider: Name of the vector DB provider (e.g., \"pinecone\")\n        operation_type: Type of operation (\"read\" or \"write\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Get trace ID from request context\n            id = request_context.get()\n            trace_id = id\n            \n            # Get provider config\n            provider_config = PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for provider: {provider}, falling back to default handling\")\n                return await func(*args, **kwargs)\n            \n            start_time = time.perf_counter()\n            \n            try:\n                result = await func(*args, **kwargs)\n            \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n        \n                if operation_type == \"write\":\n                    operation_data = provider_config[\"write_parser\"](result)\n                else:  # read\n                    operation_data = provider_config[\"read_parser\"](result)\n                \n                price_data = provider_config[\"price_calculator\"](\n                    operation_data[\"operation_type\"],\n                    operation_data[\"units\"]\n                )\n                \n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                # Extract relevant function arguments for the trace\n                # For Pinecone upsert\n                if operation_type == \"write\" and len(args) > 2:\n                    index_host = args[1]\n                    namespace = args[3]\n                    vectors_count = len(args[2]) if isinstance(args[2], list) else 0\n                    operation_details = {\n                        \"index_host\": index_host,\n                        \"namespace\": namespace,\n                        \"vectors_count\": vectors_count\n                    }\n                # For Pinecone queries\n                elif operation_type == \"read\" and len(args) >= 1:\n                    index_host = kwargs.get(\"index_host\", \"\")\n                    namespace = args[2] if len(args) > 2 else kwargs.get(\"namespace\", \"\")\n                    top_k = args[3] if len(args) > 3 else kwargs.get(\"top_k\", 0)\n                    pinecone_response = provider_config[\"response_extractor\"](result)\n                    query = kwargs.get(\"query\", \"\")\n                    operation_details = {\n                        \"index_host\": index_host,\n                        \"namespace\": namespace,\n                        \"top_k\": top_k,\n                    }\n                else:\n                    operation_details = {}\n                \n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    span_data = {\n                            \"service_provider\": provider,\n                            \"operation_type\": operation_type,\n                            \"response\": pinecone_response or \"\",\n                            \"operation_details\": operation_details,\n                            \"units\": operation_data[\"units\"],\n                            \"price\": price_data[\"price\"],\n                            \"query\" : query,\n                            \"start_time\": start_time,\n                            \"end_time\": end_time,\n                            \"response_time\": response_time,\n                            \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\")\n                        }\n                    \n                    # logger.info(f\"Span data: {span_data}\")\n                    await _LangfuseService.create_span_for_vectorDB(trace_id = trace_id, span_data=span_data, name=f\"{provider.capitalize()} {operation_type.capitalize()}\")\n                    # logger.info(f\"Span created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator\nThis code defines a decorator `vectordb_tracing` that instruments vector database API calls (Pinecone in this example) for observability, logging operation details, pricing, and sending trace data to a Langfuse service.\n",
        "size": 4697,
        "parent-class": null,
        "function_name": "vectordb_tracing"
    },
    {
        "id": "0ef48ada-3000-4280-aded-adb76e5c2265",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 1,
        "end_line": 3,
        "content": "import uuid\nfrom fastapi import Request\nfrom observe_traces.config.context_util import request_context, tracer_context, langfuse_context\nImport statements at the beginning of a FastAPI middleware function definition.\n",
        "size": 217,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "707fa1a5-e006-45af-8f77-9ad6f5e45e23",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 5,
        "end_line": 17,
        "content": "async def set_request_context(request: Request, call_next):\n    request_id = request.headers.get(\"X-Request-ID\", str(uuid.uuid4())) \n \n    if request_context.get() is None:\n        token = request_context.set(request_id)\n\n    try:\n        response = await call_next(request)\n    finally:\n        request_context.reset(token)\n        \n    response.headers[\"X-Request-ID\"] = request_id\n    return response\nFunction to set and manage request context using X-Request-ID header, generating a UUID if missing.\n",
        "size": 504,
        "parent-class": null,
        "function_name": "set_request_context"
    },
    {
        "id": "5dabcb00-25ab-4a89-9f2f-b49fd29e5a56",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 19,
        "end_line": 45,
        "content": "async def create_unified_trace(request: Request, call_next):\n    try:\n        trace_id = request.headers.get(\"X-Request-ID\", None)\n        langfuse_client = langfuse_context.get()\n        \n        if not trace_id:\n            trace = langfuse_client.trace(\n                id = request_context.get(),\n                name = f\"Trace ID {request_context.get()}\",\n            )\n            \n            token = tracer_context.set(trace)\n            \n            trace_id = trace.id\n            \n        else:\n            trace = langfuse_client.trace(id = trace_id)\n            \n        request.state.trace_id = trace_id\n        \n        response = await call_next(request)\n        \n        response.headers[\"X-Trace-ID\"] = trace_id\n    finally:\n        tracer_context.reset(token)\n    \n    return response\nFunction to create a unified trace ID using Langfuse, setting it in request state and response headers.  Handles cases with and without existing X-Request-ID.\n",
        "size": 963,
        "parent-class": null,
        "function_name": "create_unified_trace"
    },
    {
        "id": "ae32737f-6ff5-4fbe-a9ed-0ca6ad322693",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 1,
        "end_line": 5,
        "content": "import time\nfrom langfuse import Langfuse\nfrom typing import Dict, Any, Optional\nfrom observe_traces.config.context_util import langfuse_context\nfrom observe_traces.config.context_util import tracer_context, langfuse_context\nImport statements at the beginning of a Python file defining a LangfuseClient class and a _LangfuseService class.\n",
        "size": 339,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "149b85b2-f0ef-436b-b8ea-230cb137894a",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 8,
        "end_line": 21,
        "content": "def __init__(\n        self,\n        \n        langfuse_public_key: str,\n        langfuse_secret_key: str,\n        langfuse_host: str,\n        release: str\n    ):\n        self.langfuse_public_key = langfuse_public_key\n        self.langfuse_secret_key = langfuse_secret_key\n        self.langfuse_host = langfuse_host\n        self.release = release\n        self.langfuse_client = None\n        self.token = None\nConstructor for the LangfuseClient class, initializing Langfuse connection parameters.\n",
        "size": 494,
        "parent-class": "LangfuseClient",
        "function_name": "__init__"
    },
    {
        "id": "a7018c79-83dd-414d-9700-7951d73fdf2d",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 23,
        "end_line": 30,
        "content": "def initialize_langfuse_client(self):\n        self.langfuse_client = Langfuse(\n            public_key=self.langfuse_public_key,\n            secret_key=self.langfuse_secret_key,\n            host=self.langfuse_host,\n            release=self.release\n        )\n        self.token = langfuse_context.set(self.langfuse_client)\n`LangfuseClient` class method to initialize the Langfuse client and store the context token.\n",
        "size": 414,
        "parent-class": "LangfuseClient",
        "function_name": "initialize_langfuse_client"
    },
    {
        "id": "f05c5987-8e44-4fea-b485-f640289e5386",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 32,
        "end_line": 33,
        "content": "def close_langfuse_client(self):\n        langfuse_context.reset(self.token)\nMethod in LangfuseClient class to close the Langfuse client and reset the context.\n",
        "size": 159,
        "parent-class": "LangfuseClient",
        "function_name": "close_langfuse_client"
    },
    {
        "id": "b8005d3b-8634-43bd-8a39-16b262f3b2cf",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 38,
        "end_line": 80,
        "content": "async def create_generation_for_LLM(trace_id: str, generation_data: Dict[str, Any], name: str) -> Optional[str]:\n        \n        try:\n            time.sleep(10)\n            trace = tracer_context.get()\n        \n            trace.update(\n                output = generation_data[\"output\"],\n            )\n        except Exception as e:\n            return None\n        \n        langfuse_client = langfuse_context.get()\n        generation_object = langfuse_client.generation(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        generation_object.end(\n            model=generation_data[\"model_name\"],\n            input=generation_data[\"input\"],\n            output=generation_data[\"output\"],\n            usage_details={\n                \"input_token\": generation_data[\"tokens\"][\"input\"],\n                \"output_token\": generation_data[\"tokens\"][\"output\"],\n                \"total_token\": generation_data[\"tokens\"][\"total\"]\n            },\n            cost_details = {\n                \"input_cost\": generation_data[\"price\"][\"input\"],\n                \"output_cost\": generation_data[\"price\"][\"output\"],\n                \"total_cost\": generation_data[\"price\"][\"total\"]\n            },\n            metadata={\n                \"provider\": generation_data[\"service_provider\"],\n                \"cost\": generation_data[\"price\"][\"total\"],\n                \"input_token\": generation_data[\"tokens\"][\"input\"],\n                \"output_token\": generation_data[\"tokens\"][\"output\"],\n                \"total_token\": generation_data[\"tokens\"][\"total\"]\n            },\n        )\n        \n        time.sleep(10)\n        return generation_object.id\nThis code snippet is a static method within the `_LangfuseService` class.  It's responsible for creating and ending a Langfuse generation object, recording LLM generation details including input, output, usage, and cost.\n",
        "size": 1866,
        "parent-class": "_LangfuseService",
        "function_name": "create_generation_for_LLM"
    },
    {
        "id": "22f85366-c5df-4c94-b8f6-a8f0ab4dcaed",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 83,
        "end_line": 105,
        "content": "async def create_span_for_vectorDB(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n  \n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input = span_data[\"query\"],\n            output = span_data[\"response\"][0][\"text\"],\n            start_time = span_data[\"start_time\"],\n            end_time = span_data[\"end_time\"],\n            metadata = {\n                \"operation_type\": span_data[\"operation_type\"],\n                \"provider\": span_data[\"service_provider\"],\n                \"cost\": span_data[\"price\"],\n                \"read_units\": span_data[\"units\"],\n            }\n        )\n        \n        time.sleep(10)\n        return span_object.id\n`_LangfuseService` class; static method creating a Langfuse span for vector database operations.\n",
        "size": 917,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_vectorDB"
    },
    {
        "id": "8df38cc3-601d-4c3e-a6a0-5614309faecc",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 108,
        "end_line": 144,
        "content": "async def create_span_for_embedding(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n\n        try:\n            time.sleep(10)\n            trace = tracer_context.get()\n    \n            trace.update(\n                input = span_data[\"input\"],\n            )\n        except Exception as e:\n            return None\n        \n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input=span_data[\"input\"],\n            start_time=span_data[\"start_time\"],\n            end_time=span_data[\"end_time\"],\n            metadata={\n                \"provider\": span_data[\"service_provider\"],\n                \"model_name\" : span_data[\"model_name\"],\n                \"input count\" : span_data[\"input_count\"],\n                \"cost\": span_data[\"price\"][\"total\"],\n                \"token usage\" : span_data[\"tokens\"],\n                \"price\" : span_data[\"price\"],\n                \"embedding_dimensions\": span_data[\"embedding_dimensions\"],\n                \"response_time\": span_data[\"response_time\"],\n                \"timestamp\": span_data[\"timestamp\"]\n            },\n        )\n        \n        time.sleep(10)\n        return span_object.id\nMethod within a LangfuseService class that creates a span for embedding data using the Langfuse client, including error handling and metadata logging.\n",
        "size": 1439,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_embedding"
    },
    {
        "id": "53994dd8-b3ec-439a-8659-e1e120113c33",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 147,
        "end_line": 177,
        "content": "async def create_span_for_reranking(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n\n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input= {\n                \"query\": span_data[\"query\"],\n                \"documents\": span_data[\"documents\"]\n            },\n            output=span_data[\"rerank_results\"],\n            start_time=span_data[\"start_time\"],\n            end_time=span_data[\"end_time\"],\n            metadata={\n                \"provider\": span_data[\"service_provider\"],\n                \"model_name\" : span_data[\"model_name\"],\n                \"output_count\" : span_data[\"document_count\"],\n                \"cost\": span_data[\"price\"],\n                \"token usage\" : span_data[\"tokens\"][\"rerank_units\"],\n                \"response_time\": span_data[\"response_time\"],\n                \"timestamp\": span_data[\"timestamp\"],\n                \"top_n\" : span_data[\"top_n\"]\n            },\n        )\n        \n        time.sleep(10)\n        \n        return span_object.id\nThis chunk defines a static method `create_span_for_reranking` within the `_LangfuseService` class, which creates and ends a Langfuse span for reranking operations, recording relevant metadata.\n",
        "size": 1336,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_reranking"
    },
    {
        "id": "d358f772-994b-4569-8f69-f7ea151cc029",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/context_util.py",
        "file_name": "context_util.py",
        "start_line": 1,
        "end_line": 3,
        "content": "from contextvars import ContextVar\nfrom fastapi import Request\nfrom langfuse.client import StatefulTraceClient, Langfuse\nImport statements for context variables and Langfuse client.\n",
        "size": 182,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "342e26f1-a5f9-4f43-a747-94943035b684",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 1,
        "end_line": 2,
        "content": "from langfuse.client import StatefulTraceClient\nfrom observe_traces.config.context_util import request_context, tracer_context\nImport statements for Langfuse tracing client and request/tracer context.\n",
        "size": 201,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "613e1aa5-4f6d-4d8e-83e4-ef0a23958029",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 4,
        "end_line": 5,
        "content": "def get_request_id() -> str:\n    return request_context.get()\nPython functions to retrieve request ID and tracer from context objects.\n",
        "size": 135,
        "parent-class": null,
        "function_name": "get_request_id"
    },
    {
        "id": "e6a4377d-972c-4e13-8207-9117f2c62490",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 7,
        "end_line": 8,
        "content": "def get_tracer() -> StatefulTraceClient:\n    return tracer_context.get()\nFunction to retrieve a StatefulTraceClient from a context.\n",
        "size": 132,
        "parent-class": null,
        "function_name": "get_tracer"
    }
]