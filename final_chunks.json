[
    {
        "id": "84fe67f6-2e0f-4e15-b33a-38909c6070ac",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService",
        "size": 249,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "7382e40e-641c-42fa-8337-2c9bce6be460",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 9,
        "end_line": 24,
        "content": "def calculate_pinecone_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"llama-text-embed-v2\": 0.16,\n        \"multilingual-e5-large\": 0.08,\n        \"pinecone-sparse-english-v0\": 0.08\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 495,
        "parent-class": null,
        "function_name": "calculate_pinecone_price"
    },
    {
        "id": "448abb38-bcc7-4c13-80cc-d0faaf2860cb",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 27,
        "end_line": 41,
        "content": "def calculate_cohere_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"embed-english-v3.0\": 0.1,\n        \"embed-multilingual-v3.0\": 0.1,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 449,
        "parent-class": null,
        "function_name": "calculate_cohere_price"
    },
    {
        "id": "fa23c4b1-8ae2-47c6-951f-dd3c35bf12b5",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 44,
        "end_line": 59,
        "content": "def calculate_jina_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"jina-embeddings-v2-base-en\": 0.05,\n        \"jina-embeddings-v3\": 0.12,\n        \"jina-embeddings-v2-base-code\": 0.05,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 498,
        "parent-class": null,
        "function_name": "calculate_jina_price"
    },
    {
        "id": "29bc756b-e84c-41c7-8ace-62bfa0c7c912",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 62,
        "end_line": 81,
        "content": "def calculate_voyageai_price(model_name: str, tokens: int) -> Dict[str, float]:\n    pricing = {\n        \"voyage-3\": 0.06,\n        \"voyage-3-lite\": 0.02,\n        \"voyage-finance-2\": 0.12,\n        \"voyage-law-2\": 0.12,\n        \"voyage-code-2\": 0.12,\n        \"voyage-code-3\": 0.18,\n        \"voyage-3-large\": 0.18,\n        # Add more models as needed\n    }\n    \n    model_price = pricing.get(model_name, 0.0)  # Default fallback\n    total_price = (tokens / 1000000) * model_price\n    \n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 591,
        "parent-class": null,
        "function_name": "calculate_voyageai_price"
    },
    {
        "id": "19f4a508-a0a9-42da-abf3-0124744545a4",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 85,
        "end_line": 92,
        "content": "def parse_pinecone_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from pinecone response\n    if \"usage\" in response_data:\n        usage = response_data.get(\"usage\", {})\n        return {\n            \"tokens\": usage.get(\"total_tokens\", 0),\n        }\n    return {\"tokens\": 0}",
        "size": 307,
        "parent-class": null,
        "function_name": "parse_pinecone_tokens"
    },
    {
        "id": "85e7c555-8a4a-4173-a43b-425bee95a779",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 95,
        "end_line": 101,
        "content": "def parse_cohere_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from cohere response\n    meta = response_data.get(\"meta\", {})\n    billed_units = meta.get(\"billed_units\", {})\n    return {\n        \"tokens\": billed_units.get(\"input_tokens\", 0),\n    }",
        "size": 282,
        "parent-class": null,
        "function_name": "parse_cohere_tokens"
    },
    {
        "id": "e68f2453-101e-4867-84f2-744b0491907d",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 104,
        "end_line": 109,
        "content": "def parse_jina_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from jina response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }",
        "size": 225,
        "parent-class": null,
        "function_name": "parse_jina_tokens"
    },
    {
        "id": "0cad3341-7e70-417a-8680-f588b97c7210",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 112,
        "end_line": 119,
        "content": "def parse_voyageai_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from voyage response\n    usage = response_data.get(\"usage\", {})\n    print(f\"inside decode voyageai tokens : {usage}\")\n\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }",
        "size": 286,
        "parent-class": null,
        "function_name": "parse_voyageai_tokens"
    },
    {
        "id": "159fd16c-01cb-4c11-bbb5-a9e16e255303",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 146,
        "end_line": 157,
        "content": "def register_embedding_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    embeddings_extractor: Callable\n):\n    # Register a new embedding provider with configurations\n    EMBEDDING_PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        # \"embeddings_extractor\": embeddings_extractor\n    }",
        "size": 412,
        "parent-class": null,
        "function_name": "register_embedding_provider"
    },
    {
        "id": "4905ae11-07c0-4c1a-8c32-e0d5f08d1e2c",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/embed_tracer.py",
        "file_name": "embed_tracer.py",
        "start_line": 160,
        "end_line": 257,
        "content": "def embedding_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing embedding API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the embedding provider (e.g., \"pinecone\", \"cohere\", \"jina\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract model_name and inputs from args and kwargs\n            model_name = kwargs.get(\"model_name\", \"\")\n            if not model_name and len(args) > 1:\n                model_name = args[1]  # Assuming model_name is second argument\n            \n            inputs = kwargs.get(\"inputs\", kwargs.get(\"texts\", []))\n            if not inputs and len(args) > 2:\n                inputs = args[2]  # Assuming inputs/texts is third argument\n            \n            # Get request ID from context\n            id = request_context.get()\n            trace_id = id\n                \n            # Get provider config\n            provider_config = EMBEDDING_PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for embedding provider: {provider}, falling back to default handling\")\n                # return await func(*args, **kwargs)\n                raise ValueError(f\"No configuration found for embedding provider: {provider}\")\n            \n            start_time = time.perf_counter()\n            \n            try:\n                # Call the original function\n                result = await func(*args, **kwargs)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n                # Process the response based on provider\n                tokens_data = {}\n                if isinstance(result, tuple):\n                    embeddings = result[0] if len(result) > 0 else []\n                    raw_response = result[1] if len(result) > 1 else None\n                    if raw_response:\n                        tokens_data = provider_config[\"token_parser\"](raw_response)\n                else:\n                    # case when function returns entire json response\n                    raw_response = result\n                    embeddings = provider_config[\"embeddings_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n                # Calculate price if token data is available\n                price_data = {}\n                if tokens_data and \"tokens\" in tokens_data:\n                    price_data = provider_config[\"price_calculator\"](\n                        model_name, \n                        tokens_data.get(\"tokens\", 0)\n                    )\n                \n                # Set timezone to IST\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                # logger.info(f\"Embedding trace data: {trace_data}\")\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating embedding generation for trace: {trace_id}\")\n                    span_data = {\n                        \"service_provider\": provider,\n                        \"model_name\": model_name,\n                        \"input\": inputs if isinstance(inputs, list) else [inputs],\n                        \"tokens\": tokens_data,\n                        \"price\": price_data,\n                        \"input_count\": len(inputs) if isinstance(inputs, list) else 1,\n                        \"response_time\": response_time,\n                        \"start_time\": start_time,\n                        \"end_time\": end_time,\n                        \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\"),\n                        \"embedding_dimensions\": len(embeddings[0]) if embeddings and len(embeddings) > 0 else 0,\n                    }\n                    \n                    # logger.info(f\"Generation data: {span_data}\")\n                    await _LangfuseService.create_span_for_embedding(trace_id = trace_id, span_data = span_data, name=f\"{provider.capitalize()} Embeddings Generation\")\n                    # logger.info(f\"Generation created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "size": 4573,
        "parent-class": null,
        "function_name": "embedding_tracing"
    },
    {
        "id": "b7867909-3318-4999-b773-90b43834c303",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService",
        "size": 249,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "e2113d01-b282-459a-ad66-4354b0e99a0b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 9,
        "end_line": 32,
        "content": "def calculate_openai_price(model_name: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:\n\n    pricing = {\n        \"gpt-3.5-turbo\": {\"input\": 0.15, \"output\": 0.6},\n        \"gpt-4o\": {\"input\": 2.5, \"output\": 10.0},    \n        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.6},\n        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n        \"o1-2024-12-17\": {\"input\": 15, \"output\": 60},\n        'o3-mini-2025-01-31': {\"input\": 1.1, \"output\": 4.40},\n        \"o1-mini-2024-09-12x\": {\"input\": 1.1,  \"output\": 4.40},\n        # when new openai models come then just add them over here \n    }\n    \n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0})  # Default fallback\n    \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n    \n    return {\n        \"input\": input_price , #round(input_price, 6),\n        \"output\": output_price, #round(output_price, 6),\n        \"total\": total_price, #round(total_price, 6)\n    }",
        "size": 1074,
        "parent-class": null,
        "function_name": "calculate_openai_price"
    },
    {
        "id": "2a456f70-f7ac-40b0-b221-bd09cf15a3fe",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 35,
        "end_line": 54,
        "content": "def calculate_anthropic_price(model_name:str, input_tokens:int, output_tokens:int) -> Dict[str, float]:\n\n    pricing = {\n        \"claude-3-7-sonnet-20250219\" : {\"input\" : 3,\"output\" : 15},\n        \"claude-3-5-sonnet-20241022\" : {\"input\" : 3,\"output\" : 15},\n        \"claude-3-5-haiku-20241022\" : {\"input\" : 0.80,\"output\" : 4},\n        \"claude-3-opus-20240229\" : {\"input\" : 15,\"output\" : 75},\n        \"claude-3-haiku-20240307\" : {\"input\" : 0.25,\"output\" : 1.25},\n    }\n\n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0}) #default fallback \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n\n    return {\n        \"input\": input_price, #round(input_price, 6),\n        \"output\": output_price,#round(output_price, 6),\n        \"total\": total_price, #round(total_price, 6)\n    }",
        "size": 928,
        "parent-class": null,
        "function_name": "calculate_anthropic_price"
    },
    {
        "id": "fe969fb8-b543-4afc-a4d1-9b2e954bc352",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 57,
        "end_line": 79,
        "content": "def calculate_groq_price(model_name: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:\n\n    pricing = {\n        \"llama-3.3-70b-versatile\": {\"input\": 0.59, \"output\": 0.79},\n        \"gemma2-9b-it\": {\"input\": 0.2, \"output\": 0.2},\n        \"llama-3.1-8b-instant\": {\"input\": 0.05, \"output\": 0.08},\n        \"llama3-70b-8192\": {\"input\": 0.59, \"output\": 0.79},\n        \"llama-guard-3-8b\": {\"input\": 0.2, \"output\": 0.2},\n        \"llama3-8b-8192\": {\"input\": 0.05, \"output\": 0.08},\n        \"mixtral-8x7b-32768\": {\"input\": 0.24, \"output\": 0.24},\n    }\n    \n    model_pricing = pricing.get(model_name, {\"input\": 0.0, \"output\": 0.0})  # Default fallback\n    \n    input_price = (input_tokens / 1000000) * model_pricing[\"input\"]\n    output_price = (output_tokens / 1000000) * model_pricing[\"output\"]\n    total_price = input_price + output_price\n    \n    return {\n        \"input\": round(input_price, 6),\n        \"output\": round(output_price, 6),\n        \"total\": round(total_price, 6)\n    }",
        "size": 987,
        "parent-class": null,
        "function_name": "calculate_groq_price"
    },
    {
        "id": "a0d425aa-b045-4821-9627-1095c4970dc6",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 83,
        "end_line": 89,
        "content": "def parse_openai_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"input\": usage.get(\"prompt_tokens\", 0),\n        \"output\": usage.get(\"completion_tokens\", 0),\n        \"total\": usage.get(\"total_tokens\", 0)\n    }",
        "size": 282,
        "parent-class": null,
        "function_name": "parse_openai_tokens"
    },
    {
        "id": "7d4c0a56-19fd-4358-a3f3-5df0a5625790",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 92,
        "end_line": 98,
        "content": "def parse_groq_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"input\": usage.get(\"prompt_tokens\", 0),\n        \"output\": usage.get(\"completion_tokens\", 0),\n        \"total\": usage.get(\"total_tokens\", 0)\n    }",
        "size": 280,
        "parent-class": null,
        "function_name": "parse_groq_tokens"
    },
    {
        "id": "8eb5e2d2-b4cb-40b0-be54-1092f7ca1670",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 100,
        "end_line": 108,
        "content": "def parse_anthropic_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    input_tokens = usage.get(\"input_tokens\", 0)\n    output_tokens = usage.get(\"output_tokens\", 0)\n    return {\n        \"input\": input_tokens,\n        \"output\": output_tokens,\n        \"total\": input_tokens + output_tokens\n    }",
        "size": 346,
        "parent-class": null,
        "function_name": "parse_anthropic_tokens"
    },
    {
        "id": "ae670f64-e2ef-4d2c-9af1-04074aabd791",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 133,
        "end_line": 144,
        "content": "def register_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    response_extractor: Callable\n):\n    #Register a new LLM provider with configurations here\n    PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        \"response_extractor\": response_extractor\n    }",
        "size": 382,
        "parent-class": null,
        "function_name": "register_provider"
    },
    {
        "id": "9057e405-2c0b-4e52-9c7f-0bfde2c17061",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/llm_tracer.py",
        "file_name": "llm_tracer.py",
        "start_line": 147,
        "end_line": 230,
        "content": "def llm_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing LLM API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the LLM provider (e.g., \"openai\", \"groq\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(self, model_name, system_prompt, user_prompt, user_query, **params):\n            # Generate trace ID if not provided\n            # if not trace_id:\n\n            id = request_context.get()\n            trace_id = id #str(uuid.uuid4())\n                \n            # Get provider config\n            provider_config = PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for provider: {provider}, falling back to default handling\")\n                return await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n            \n           \n            start_time = time.perf_counter()\n            \n            try:\n            \n                result = await func(self, model_name, system_prompt, user_prompt, user_query, **params)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n               \n                if isinstance(result, tuple):\n                    response_data = result[0] if len(result) > 0 else None\n                    raw_response = result[1] if len(result) > 1 else None\n                    # jis hisab se response de rha he funciton ye change krna pdega \n                    llm_response = response_data\n                    tokens_data = provider_config[\"token_parser\"](raw_response) if raw_response else {}\n                else:\n                    # case when function returns entire json response from llm\n                    raw_response = result\n                    llm_response = provider_config[\"response_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n\n                price_data = provider_config[\"price_calculator\"](\n                    model_name, \n                    tokens_data.get(\"input\", 0), \n                    tokens_data.get(\"output\", 0)\n                )\n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    generation_data = {\n                            \"model_name\": model_name,\n                            \"service_provider\": provider,\n                            \"input\": user_query,\n                            \"output\": llm_response,\n                            \"tokens\": tokens_data,\n                            \"price\": price_data,\n                            \"system_prompt\": system_prompt,\n                            \"start_time\": datetime.fromtimestamp(start_time),\n                            \"end_time\": datetime.fromtimestamp(end_time),\n                        }\n                    \n                    # logger.info(f\"Generation data: {generation_data}\")\n                    \n                    await _LangfuseService.create_generation_for_LLM(trace_id, generation_data, f\"{provider.capitalize()} Generation\")\n                    # logger.info(f\"Generation created with ID: {generation_id}\")\n                \n                except Exception as e:\n                    raise e\n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n               \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "size": 3739,
        "parent-class": null,
        "function_name": "llm_tracing"
    },
    {
        "id": "408aaa34-6553-490c-8d24-e5fef60ed3e4",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any, Callable\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService",
        "size": 249,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "24551203-d453-4b98-8159-2136594c55a5",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 9,
        "end_line": 20,
        "content": "def calculate_pinecone_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"pinecone-rerank-v0\": 0.10,  # Example: $0.10 per 1k rerank units\n    }\n    rerank_units = tokens_data.get(\"rerank_units\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (rerank_units / 1000) * model_price\n    return {\n        \"rerank_units\": rerank_units,\n        \"price_per_1K\": model_price,\n        \"total\": total_price\n    }",
        "size": 476,
        "parent-class": null,
        "function_name": "calculate_pinecone_rerank_price"
    },
    {
        "id": "c969e77f-1c60-49f0-bce2-c3e8113df1e2",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 23,
        "end_line": 34,
        "content": "def calculate_cohere_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"rerank-english-v3.0\": 0.15,  # Example: $0.15 per search unit\n    }\n    search_units = tokens_data.get(\"search_units\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = search_units * model_price\n    return {\n        \"search_units\": search_units,\n        \"price_per_unit\": model_price,\n        \"total\": total_price\n    }",
        "size": 464,
        "parent-class": null,
        "function_name": "calculate_cohere_rerank_price"
    },
    {
        "id": "508ea989-b5de-4ec8-854c-b9549ddd1d00",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 37,
        "end_line": 48,
        "content": "def calculate_jina_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"jina-rerank-v1-tiny-en\": 0.08,  # Example: $0.08 per 1M tokens\n    }\n    tokens = tokens_data.get(\"tokens\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (tokens / 1000000) * model_price\n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 443,
        "parent-class": null,
        "function_name": "calculate_jina_rerank_price"
    },
    {
        "id": "faf7795e-5bf4-4b23-bfb8-eb7dadc4feff",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 51,
        "end_line": 62,
        "content": "def calculate_voyage_rerank_price(model_name: str, tokens_data: Dict[str, int]) -> Dict[str, float]:\n    pricing = {\n        \"voyage-rerank-v1\": 0.12,  # Example: $0.12 per 1M tokens\n    }\n    tokens = tokens_data.get(\"tokens\", 0)\n    model_price = pricing.get(model_name, 0.0)\n    total_price = (tokens / 1000000) * model_price\n    return {\n        \"tokens\": tokens,\n        \"price_per_1M\": model_price,\n        \"total\": total_price\n    }",
        "size": 439,
        "parent-class": null,
        "function_name": "calculate_voyage_rerank_price"
    },
    {
        "id": "2f0cfd2a-0563-42c7-b9b5-034d74b2b10b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 66,
        "end_line": 68,
        "content": "def parse_pinecone_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    return {\"rerank_units\": usage.get(\"rerank_units\", 0)}",
        "size": 183,
        "parent-class": null,
        "function_name": "parse_pinecone_rerank_tokens"
    },
    {
        "id": "39ad6a46-f604-4924-aa20-24c56cc4e0e5",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 71,
        "end_line": 77,
        "content": "def parse_cohere_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from cohere response\n    meta = response_data.get(\"meta\", {})\n    billed_units = meta.get(\"billed_units\", {})\n    return {\n        \"search_units\": billed_units.get(\"search_units\", 0),\n    }",
        "size": 295,
        "parent-class": null,
        "function_name": "parse_cohere_rerank_tokens"
    },
    {
        "id": "0767c849-f55d-4e89-889e-83307085d6e1",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 80,
        "end_line": 85,
        "content": "def parse_jina_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from jina response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }",
        "size": 232,
        "parent-class": null,
        "function_name": "parse_jina_rerank_tokens"
    },
    {
        "id": "d03d1ca8-91d6-432a-9ec7-0d0898d069b0",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 88,
        "end_line": 93,
        "content": "def parse_voyage_rerank_tokens(response_data: Dict[str, Any]) -> Dict[str, int]:\n    # Extract usage data from voyage response\n    usage = response_data.get(\"usage\", {})\n    return {\n        \"tokens\": usage.get(\"total_tokens\", 0),\n    }",
        "size": 236,
        "parent-class": null,
        "function_name": "parse_voyage_rerank_tokens"
    },
    {
        "id": "7f6568e0-dee7-4e07-a1b2-4f627e3aed2b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 120,
        "end_line": 131,
        "content": "def register_reranking_provider(\n    provider_name: str, \n    token_parser: Callable, \n    price_calculator: Callable, \n    rerank_results_extractor: Callable\n):\n    # Register a new reranking provider with configurations\n    RERANKING_PROVIDER_CONFIGS[provider_name] = {\n        \"token_parser\": token_parser,\n        \"price_calculator\": price_calculator,\n        \"rerank_results_extractor\": rerank_results_extractor\n    }",
        "size": 422,
        "parent-class": null,
        "function_name": "register_reranking_provider"
    },
    {
        "id": "4c8e117d-cce4-41e7-8f28-6befba045e8e",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/rerank_tracer.py",
        "file_name": "rerank_tracer.py",
        "start_line": 134,
        "end_line": 248,
        "content": "def reranking_tracing(provider: str):\n    \"\"\"\n    Decorator for tracing reranking API calls with provider-specific handling\n    \n    Args:\n        provider: Name of the reranking provider (e.g., \"pinecone\", \"cohere\", \"jina\", \"voyage\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extract model_name, query, and documents from args and kwargs\n            model_name = kwargs.get(\"model_name\", \"\")\n            if not model_name and len(args) > 1:\n                model_name = args[1]  # Assuming model_name is second argument\n            \n            query = kwargs.get(\"query\", \"\")\n            if not query and len(args) > 2:\n                query = args[2]  # Assuming query is third argument\n            \n            documents = kwargs.get(\"documents\", [])\n            if not documents and len(args) > 3:\n                documents = args[3]  # Assuming documents is fourth argument\n\n            top_n = kwargs.get(\"top_n\", 0)\n            if not top_n and len(args) > 4:\n                top_n = args[4]\n            \n            # Get request ID from context\n            id = request_context.get()\n            trace_id = id\n                \n            # Get provider config\n            provider_config = RERANKING_PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for reranking provider: {provider}, falling back to default handling\")\n                return await func(*args, **kwargs)\n            \n            start_time = time.perf_counter()\n            ist = timezone(timedelta(hours=5, minutes=30))\n            \n            try:\n                # Call the original function\n                result = await func(*args, **kwargs)\n                \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n                \n                # Process the response based on provider\n                tokens_data = {}\n                if isinstance(result, tuple):\n                    rerank_results = result[0] if len(result) > 0 else []\n                    raw_response = result[1] if len(result) > 1 else None\n                    if raw_response:\n                        tokens_data = provider_config[\"token_parser\"](raw_response)\n                else:\n                    # case when function returns entire json response\n                    raw_response = result\n                    rerank_results = provider_config[\"rerank_results_extractor\"](raw_response)\n                    tokens_data = provider_config[\"token_parser\"](raw_response)\n                \n                \n                rerank_results = []\n                for doc in result[\"data\"]:\n                    rerank_results.append(\n                        documents[doc[\"index\"]]\n                    )\n                \n                # Calculate price if token data is available\n                price_data = {}\n                if tokens_data and \"tokens\" in tokens_data:\n                    price_data = provider_config[\"price_calculator\"](\n                        model_name, \n                        tokens_data.get(\"tokens\", 0)\n                    )\n\n                # raw_response = result\n                # tokens_data = provider_config[\"token_parser\"](raw_response)\n                # rerank_results = provider_config[\"rerank_results_extractor\"](raw_response)\n                # price_data = provider_config[\"price_calculator\"](model_name, tokens_data)\n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    span_data = {\n                        \"service_provider\": provider,\n                        \"model_name\": model_name,\n                        \"tokens\": tokens_data,\n                        \"price\": price_data,\n                        \"query\": query,\n                        \"documents\": documents,\n                        \"document_count\": len(documents),\n                        \"top_n\": top_n,\n                        \"rerank_results\": rerank_results,\n                        \"response_time\": response_time,\n                        \"start_time\" : start_time,\n                        \"end_time\" : end_time,\n                        \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\")\n                    }\n                    \n                    # logger.info(f\"Span data: {span_data}\")\n                    await _LangfuseService.create_span_for_reranking(trace_id = trace_id, span_data=span_data, name=f\"{provider.capitalize()} Reranking\")\n                    # logger.info(f\"Span created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                \n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "size": 5098,
        "parent-class": null,
        "function_name": "reranking_tracing"
    },
    {
        "id": "606513e4-8a5f-4a9c-982c-2a77f093845d",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 1,
        "end_line": 6,
        "content": "import functools\nimport time\nfrom typing import Dict, Any\nfrom observe_traces.config.context_util import request_context\nfrom datetime import datetime, timezone, timedelta\nfrom observe_traces.config.langfuse_service import _LangfuseService",
        "size": 239,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "9206ca4b-0388-43bf-98cd-c3cfcf864362",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 10,
        "end_line": 21,
        "content": "def calculate_pinecone_price(operation_type: str, units: int) -> Dict[str, float]:\n    pricing = {\n        \"read\": 16.0,  # $16 per million read units\n        \"write\": 4.0,  # $4 per million write units\n    }\n    \n    price = (units / 1000000) * pricing[operation_type]\n    \n    return {\n        \"units\": units,\n        \"price\": price\n    }",
        "size": 340,
        "parent-class": null,
        "function_name": "calculate_pinecone_price"
    },
    {
        "id": "05690de6-f111-40c5-a554-6a6a4e1fe450",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 24,
        "end_line": 28,
        "content": "def parse_pinecone_write_response(response_data: Dict[str, Any]) -> Dict[str, int]:\n    return {\n        \"operation_type\": \"write\",\n        \"units\": response_data.get(\"upsertedCount\", 0)\n    }",
        "size": 192,
        "parent-class": null,
        "function_name": "parse_pinecone_write_response"
    },
    {
        "id": "6dc0a01b-54a0-4494-812b-ad446f1999f0",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 30,
        "end_line": 36,
        "content": "def parse_pinecone_read_response(response_data: Dict[str, Any]) -> Dict[str, int]:\n    usage = response_data.get(\"usage\", {})\n    # print(f\"usage in pinecone parse read response : {usage}\")\n    return {\n        \"operation_type\": \"read\",\n        \"units\": usage.get(\"readUnits\", 0)\n    }",
        "size": 285,
        "parent-class": null,
        "function_name": "parse_pinecone_read_response"
    },
    {
        "id": "734c6f7f-362b-48bf-b05d-ea99123fce79",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/tracer/vector_tracer.py",
        "file_name": "vector_tracer.py",
        "start_line": 56,
        "end_line": 155,
        "content": "def vectordb_tracing(provider: str, operation_type: str):\n    \"\"\"\n    Decorator for tracing Vector DB API calls\n    \n    Args:\n        provider: Name of the vector DB provider (e.g., \"pinecone\")\n        operation_type: Type of operation (\"read\" or \"write\")\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Get trace ID from request context\n            id = request_context.get()\n            trace_id = id\n            \n            # Get provider config\n            provider_config = PROVIDER_CONFIGS.get(provider, {})\n            if not provider_config:\n                # logger.warning(f\"No config found for provider: {provider}, falling back to default handling\")\n                return await func(*args, **kwargs)\n            \n            start_time = time.perf_counter()\n            \n            try:\n                result = await func(*args, **kwargs)\n            \n                end_time = time.perf_counter()\n                response_time = end_time - start_time\n        \n                if operation_type == \"write\":\n                    operation_data = provider_config[\"write_parser\"](result)\n                else:  # read\n                    operation_data = provider_config[\"read_parser\"](result)\n                \n                price_data = provider_config[\"price_calculator\"](\n                    operation_data[\"operation_type\"],\n                    operation_data[\"units\"]\n                )\n                \n                ist = timezone(timedelta(hours=5, minutes=30))\n                \n                # Extract relevant function arguments for the trace\n                # For Pinecone upsert\n                if operation_type == \"write\" and len(args) > 2:\n                    index_host = args[1]\n                    namespace = args[3]\n                    vectors_count = len(args[2]) if isinstance(args[2], list) else 0\n                    operation_details = {\n                        \"index_host\": index_host,\n                        \"namespace\": namespace,\n                        \"vectors_count\": vectors_count\n                    }\n                # For Pinecone queries\n                elif operation_type == \"read\" and len(args) >= 1:\n                    index_host = kwargs.get(\"index_host\", \"\")\n                    namespace = args[2] if len(args) > 2 else kwargs.get(\"namespace\", \"\")\n                    top_k = args[3] if len(args) > 3 else kwargs.get(\"top_k\", 0)\n                    pinecone_response = provider_config[\"response_extractor\"](result)\n                    query = kwargs.get(\"query\", \"\")\n                    operation_details = {\n                        \"index_host\": index_host,\n                        \"namespace\": namespace,\n                        \"top_k\": top_k,\n                    }\n                else:\n                    operation_details = {}\n                \n                \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY BELOW ###\n                try:\n                    # logger.info(f\"Creating generation for trace: {trace_id}\")\n                    span_data = {\n                            \"service_provider\": provider,\n                            \"operation_type\": operation_type,\n                            \"response\": pinecone_response or \"\",\n                            \"operation_details\": operation_details,\n                            \"units\": operation_data[\"units\"],\n                            \"price\": price_data[\"price\"],\n                            \"query\" : query,\n                            \"start_time\": start_time,\n                            \"end_time\": end_time,\n                            \"response_time\": response_time,\n                            \"timestamp\": datetime.now(ist).strftime(\"%Y-%m-%d %H:%M:%S\")\n                        }\n                    \n                    # logger.info(f\"Span data: {span_data}\")\n                    await _LangfuseService.create_span_for_vectorDB(trace_id = trace_id, span_data=span_data, name=f\"{provider.capitalize()} {operation_type.capitalize()}\")\n                    # logger.info(f\"Span created with ID: {span_id}\")\n                \n                except Exception as e:\n                    raise e\n                    \n                ### ADD YOUR CUSTOM LOGIC FOR OBSERVABILITY ABOVE ###\n                return result\n                \n            except Exception as e:\n                raise e\n                \n        return wrapper\n    return decorator",
        "size": 4476,
        "parent-class": null,
        "function_name": "vectordb_tracing"
    },
    {
        "id": "d2512ca9-256e-4cf5-843e-be318113ff6b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 1,
        "end_line": 3,
        "content": "import uuid\nfrom fastapi import Request\nfrom observe_traces.config.context_util import request_context, tracer_context, langfuse_context",
        "size": 136,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "208690b9-9044-4057-aa66-766bd63a639e",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 5,
        "end_line": 17,
        "content": "async def set_request_context(request: Request, call_next):\n    request_id = request.headers.get(\"X-Request-ID\", str(uuid.uuid4())) \n \n    if request_context.get() is None:\n        token = request_context.set(request_id)\n\n    try:\n        response = await call_next(request)\n    finally:\n        request_context.reset(token)\n        \n    response.headers[\"X-Request-ID\"] = request_id\n    return response",
        "size": 403,
        "parent-class": null,
        "function_name": "set_request_context"
    },
    {
        "id": "70d88cf5-f6ce-4f62-bb77-ed6084cdcfd3",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/middleware/middleware.py",
        "file_name": "middleware.py",
        "start_line": 19,
        "end_line": 45,
        "content": "async def create_unified_trace(request: Request, call_next):\n    try:\n        trace_id = request.headers.get(\"X-Request-ID\", None)\n        langfuse_client = langfuse_context.get()\n        \n        if not trace_id:\n            trace = langfuse_client.trace(\n                id = request_context.get(),\n                name = f\"Trace ID {request_context.get()}\",\n            )\n            \n            token = tracer_context.set(trace)\n            \n            trace_id = trace.id\n            \n        else:\n            trace = langfuse_client.trace(id = trace_id)\n            \n        request.state.trace_id = trace_id\n        \n        response = await call_next(request)\n        \n        response.headers[\"X-Trace-ID\"] = trace_id\n    finally:\n        tracer_context.reset(token)\n    \n    return response",
        "size": 803,
        "parent-class": null,
        "function_name": "create_unified_trace"
    },
    {
        "id": "c2f2248a-3b06-4944-a45d-0fbf785e280d",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 1,
        "end_line": 5,
        "content": "import time\nfrom langfuse import Langfuse\nfrom typing import Dict, Any, Optional\nfrom observe_traces.config.context_util import langfuse_context\nfrom observe_traces.config.context_util import tracer_context, langfuse_context",
        "size": 224,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "f41b103c-3a02-4710-a13a-8e41ce48c9db",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 8,
        "end_line": 21,
        "content": "def __init__(\n        self,\n        \n        langfuse_public_key: str,\n        langfuse_secret_key: str,\n        langfuse_host: str,\n        release: str\n    ):\n        self.langfuse_public_key = langfuse_public_key\n        self.langfuse_secret_key = langfuse_secret_key\n        self.langfuse_host = langfuse_host\n        self.release = release\n        self.langfuse_client = None\n        self.token = None",
        "size": 406,
        "parent-class": "LangfuseClient",
        "function_name": "__init__"
    },
    {
        "id": "334fdd65-6228-4efc-8c33-e2e1e6f14572",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 23,
        "end_line": 30,
        "content": "def initialize_langfuse_client(self):\n        self.langfuse_client = Langfuse(\n            public_key=self.langfuse_public_key,\n            secret_key=self.langfuse_secret_key,\n            host=self.langfuse_host,\n            release=self.release\n        )\n        self.token = langfuse_context.set(self.langfuse_client)",
        "size": 320,
        "parent-class": "LangfuseClient",
        "function_name": "initialize_langfuse_client"
    },
    {
        "id": "fe021af7-8897-4f8d-89d3-8102397c2922",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 32,
        "end_line": 33,
        "content": "def close_langfuse_client(self):\n        langfuse_context.reset(self.token)",
        "size": 75,
        "parent-class": "LangfuseClient",
        "function_name": "close_langfuse_client"
    },
    {
        "id": "0aa0ebf7-990e-4574-bea2-fa335fa189fe",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 38,
        "end_line": 80,
        "content": "async def create_generation_for_LLM(trace_id: str, generation_data: Dict[str, Any], name: str) -> Optional[str]:\n        \n        try:\n            time.sleep(10)\n            trace = tracer_context.get()\n        \n            trace.update(\n                output = generation_data[\"output\"],\n            )\n        except Exception as e:\n            return None\n        \n        langfuse_client = langfuse_context.get()\n        generation_object = langfuse_client.generation(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        generation_object.end(\n            model=generation_data[\"model_name\"],\n            input=generation_data[\"input\"],\n            output=generation_data[\"output\"],\n            usage_details={\n                \"input_token\": generation_data[\"tokens\"][\"input\"],\n                \"output_token\": generation_data[\"tokens\"][\"output\"],\n                \"total_token\": generation_data[\"tokens\"][\"total\"]\n            },\n            cost_details = {\n                \"input_cost\": generation_data[\"price\"][\"input\"],\n                \"output_cost\": generation_data[\"price\"][\"output\"],\n                \"total_cost\": generation_data[\"price\"][\"total\"]\n            },\n            metadata={\n                \"provider\": generation_data[\"service_provider\"],\n                \"cost\": generation_data[\"price\"][\"total\"],\n                \"input_token\": generation_data[\"tokens\"][\"input\"],\n                \"output_token\": generation_data[\"tokens\"][\"output\"],\n                \"total_token\": generation_data[\"tokens\"][\"total\"]\n            },\n        )\n        \n        time.sleep(10)\n        return generation_object.id",
        "size": 1644,
        "parent-class": "_LangfuseService",
        "function_name": "create_generation_for_LLM"
    },
    {
        "id": "963db96c-cb8e-43a7-9fd5-b9e5b63c344a",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 83,
        "end_line": 105,
        "content": "async def create_span_for_vectorDB(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n  \n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input = span_data[\"query\"],\n            output = span_data[\"response\"][0][\"text\"],\n            start_time = span_data[\"start_time\"],\n            end_time = span_data[\"end_time\"],\n            metadata = {\n                \"operation_type\": span_data[\"operation_type\"],\n                \"provider\": span_data[\"service_provider\"],\n                \"cost\": span_data[\"price\"],\n                \"read_units\": span_data[\"units\"],\n            }\n        )\n        \n        time.sleep(10)\n        return span_object.id",
        "size": 819,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_vectorDB"
    },
    {
        "id": "160f86d9-f570-4161-b53a-dbdf895d70ea",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 108,
        "end_line": 144,
        "content": "async def create_span_for_embedding(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n\n        try:\n            time.sleep(10)\n            trace = tracer_context.get()\n    \n            trace.update(\n                input = span_data[\"input\"],\n            )\n        except Exception as e:\n            return None\n        \n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input=span_data[\"input\"],\n            start_time=span_data[\"start_time\"],\n            end_time=span_data[\"end_time\"],\n            metadata={\n                \"provider\": span_data[\"service_provider\"],\n                \"model_name\" : span_data[\"model_name\"],\n                \"input count\" : span_data[\"input_count\"],\n                \"cost\": span_data[\"price\"][\"total\"],\n                \"token usage\" : span_data[\"tokens\"],\n                \"price\" : span_data[\"price\"],\n                \"embedding_dimensions\": span_data[\"embedding_dimensions\"],\n                \"response_time\": span_data[\"response_time\"],\n                \"timestamp\": span_data[\"timestamp\"]\n            },\n        )\n        \n        time.sleep(10)\n        return span_object.id",
        "size": 1287,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_embedding"
    },
    {
        "id": "ce922faf-3bef-49ae-b279-b657c54cf76d",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/langfuse_service.py",
        "file_name": "langfuse_service.py",
        "start_line": 147,
        "end_line": 177,
        "content": "async def create_span_for_reranking(trace_id: str, span_data: Dict[str, Any], name: str) -> Optional[str]:\n\n        langfuse_client = langfuse_context.get()\n        span_object = langfuse_client.span(\n            trace_id = trace_id,\n            name=name,\n        )\n        \n        span_object.end(\n            input= {\n                \"query\": span_data[\"query\"],\n                \"documents\": span_data[\"documents\"]\n            },\n            output=span_data[\"rerank_results\"],\n            start_time=span_data[\"start_time\"],\n            end_time=span_data[\"end_time\"],\n            metadata={\n                \"provider\": span_data[\"service_provider\"],\n                \"model_name\" : span_data[\"model_name\"],\n                \"output_count\" : span_data[\"document_count\"],\n                \"cost\": span_data[\"price\"],\n                \"token usage\" : span_data[\"tokens\"][\"rerank_units\"],\n                \"response_time\": span_data[\"response_time\"],\n                \"timestamp\": span_data[\"timestamp\"],\n                \"top_n\" : span_data[\"top_n\"]\n            },\n        )\n        \n        time.sleep(10)\n        \n        return span_object.id",
        "size": 1141,
        "parent-class": "_LangfuseService",
        "function_name": "create_span_for_reranking"
    },
    {
        "id": "dcf8f0a2-7dcb-4235-9b39-08aa8cf686a8",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/context_util.py",
        "file_name": "context_util.py",
        "start_line": 1,
        "end_line": 3,
        "content": "from contextvars import ContextVar\nfrom fastapi import Request\nfrom langfuse.client import StatefulTraceClient, Langfuse",
        "size": 120,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "08f4fd3a-e667-493c-a427-4b12651e2f9b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 1,
        "end_line": 2,
        "content": "from langfuse.client import StatefulTraceClient\nfrom observe_traces.config.context_util import request_context, tracer_context",
        "size": 126,
        "parent-class": null,
        "function_name": null
    },
    {
        "id": "12a4485f-e947-4086-a766-b35ef6b55d62",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 4,
        "end_line": 5,
        "content": "def get_request_id() -> str:\n    return request_context.get()",
        "size": 61,
        "parent-class": null,
        "function_name": "get_request_id"
    },
    {
        "id": "6f601cd2-90ea-4e04-b25c-509cedb9805b",
        "file_path": "/Users/tapankheni/Developer/POC-SWE-RAG/observe_traces/config/utils.py",
        "file_name": "utils.py",
        "start_line": 7,
        "end_line": 8,
        "content": "def get_tracer() -> StatefulTraceClient:\n    return tracer_context.get()",
        "size": 72,
        "parent-class": null,
        "function_name": "get_tracer"
    }
]